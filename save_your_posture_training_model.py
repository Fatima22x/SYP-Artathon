# -*- coding: utf-8 -*-
"""Save_your_posture_training_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oa-xAaC68G5QXt7RNUjXYZlyzW6WqhUC

# CNN Image classification with Keras

## Load needed packages
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import os

import random
import numpy as np
import keras

import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow

from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Activation
from keras.layers import Conv2D, MaxPooling2D
from keras.models import Model
from keras.callbacks import ModelCheckpoint

"""## Get The data"""

# Extract the data from the zip file
!unzip "Our_dataset.zip"

"""### Load the dataset"""

root = 'Our_dataset'
train_split, val_split = 0.7, 0.15

categories = [  x[0] for x in os.walk(root) if x[0]  ]   [1:]

print(categories)

"""This function is useful for pre-processing the data into an image and input vector."""

# helper function to load image and return it and input vector
def get_image(path):
    img = image.load_img(path, target_size=(200, 200))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)
    return x

"""Load all the images from root folder"""

for c, category in enumerate(categories):
  print(c, "---", category)

data = []

for c, category in enumerate(categories):
    images = [os.path.join(dp, f) for dp, dn, filenames 
              in os.walk(category) for f in filenames 
              if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]

              
    for img_path in images:
        img = get_image(img_path)
        data.append({'x':np.array(img[0]), 'y':c})

# count the number of classes
num_classes = len(categories)

num_classes

"""## Prepare the Data

create training / validation / test split (70%, 15%, 15%)
"""

train_split + val_split

import random

random.shuffle(data)

idX_val = int(train_split * len(data))

idX_test = int((train_split + val_split) * len(data))

train = data[:idX_val]

val = data[idX_val:idX_test]


test = data[idX_test:]

"""Separate data for labels."""

X_train, y_train = np.array([t["x"] for t in train]), [t["y"] for t in train]
X_val, y_val = np.array([t["x"] for t in val]), [t["y"] for t in val]
X_test, y_test = np.array([t["x"] for t in test]), [t["y"] for t in test]
print(y_test)

"""Pre-process the data as before by making sure it's float32 and normalized between 0 and 1."""

# normalize data
X_train = X_train.astype('float32') / 255.
X_val = X_val.astype('float32') / 255.
X_test = X_test.astype('float32') / 255.

# convert labels to one-hot vectors
y_train = keras.utils.to_categorical(y_train, num_classes)
y_val = keras.utils.to_categorical(y_val, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
print(y_test.shape)

"""Let's get a summary of what we have."""

# summary
print("finished loading %d images from %d categories"%(len(data), num_classes))
print("train / validation / test split: %d, %d, %d"%(len(X_train), len(X_val), len(X_test)))
print("training data shape: ", X_train.shape)
print("training labels shape: ", y_train.shape)

"""## Visualize your data"""

images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]

idx = [int(len(images) * random.random()) for i in range(8)]


imgs = [image.load_img(images[i], target_size=(224, 224)) for i in idx]

concat_image = np.concatenate([np.asarray(img) for img in imgs], axis=1)

plt.figure(figsize=(20,4))
plt.imshow(concat_image)

"""## Train a convolutional Neural Net from scratch


"""

# TODO: Build your own neural Network
model = Sequential()


model.add( Conv2D(32, (3,3), input_shape=X_train.shape[1:]))
model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(32, (3,3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))


model.add(Conv2D(32, (3,3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(64, (3,3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Dropout(0.25))
# Fully connected layers

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))

#output layer
model.add(Dense(num_classes))
model.add(Activation('sigmoid'))
model.summary()

# TODO: compile the model to use categorical_cross-entropy loss function and adadelta optimizer
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

checkpoint = ModelCheckpoint('model-{epoch:03d}-{accuracy:03f}-{val_accuracy:03f}.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  


history = model.fit(X_train, y_train,
                    batch_size=20,
                    epochs=100,
                    validation_data=(X_val, y_val), callbacks=[checkpoint])

"""Let's plot the validation loss and validation accuracy over time."""

from sklearn.metrics import roc_auc_score, roc_curve, plot_confusion_matrix, confusion_matrix, plot_roc_curve

y_predic = model.predict(X_test)

fig = plt.figure(figsize=(16,4))
ax = fig.add_subplot(121)
ax.plot(history.history["val_loss"])
ax.set_title("validation loss")
ax.set_xlabel("epochs")

ax2 = fig.add_subplot(122)
ax2.plot(history.history["val_accuracy"])
ax2.set_title("validation accuracy")
ax2.set_xlabel("epochs")
ax2.set_ylim(0, 1)

plt.show()

loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print('Test loss:', loss)
print('Test accuracy:', accuracy)

loss, accuracy = model.evaluate(X_train, y_train, verbose=0)
print('Train loss:', loss)
print('Train accuracy:', accuracy)

n = 12
"match" if np.argmax(model.predict(X_train[n].reshape(1,200, 200, 3))) == np.argmax(y_train[n]) else "not match"

n = 24
"match" if np.argmax(model.predict(X_test[n].reshape(1,200, 200, 3))) == np.argmax(y_test[n]) else "not match"

model_name = 'Our_dataset.h5'
model.save(model_name)

predict_train = model.predict(X_train)
predict_test = model.predict(X_test)

print(predict_train)
print(predict_train.shape)

print(y_train)
print(y_train.shape)

from sklearn.metrics import classification_report, confusion_matrix
predict_train = model.predict(X_train)

predict_train = predict_train.astype(int)

print(confusion_matrix(y_train[:, [0]], predict_train[:, [0]]))

print(classification_report(y_train[:, [0]], predict_train[:, [0]]))

"""### **Plotting**"""

from sklearn.metrics import roc_curve, auc , roc_auc_score

y_pred = model.predict(X_test)

print(y_pred.shape)

print("y predict:\n", y_pred)
print("y test:\n", y_test)

"""plot roc curve"""

from sklearn.metrics import roc_curve, auc
 
n_classes=2
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
 fpr[i], tpr[i], _ = roc_curve(y_test[:, i], predict_test[:, i])
 roc_auc[i] = auc(fpr[i], tpr[i])
 
# Plot of a ROC curve for a specific class
for i in range(n_classes):
 plt.figure()
 plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])
 plt.plot([0, 1], [0, 1], 'k--')
 plt.xlim([0.0, 1.0])
 plt.ylim([0.0, 1.05])
 plt.xlabel('False Positive Rate')
 plt.ylabel('True Positive Rate')
 plt.title('Receiver operating characteristic example')
 plt.legend(loc="lower right")
 plt.show()

"""import imag"""

import cv2
im_color = cv2.imread('/content/00000299.png')
im_color =np.expand_dims(im_color, axis=0)
predictions = model.predict(im_color)
predict_class = np.argmax(predictions)
print("predict_class ",predict_class)